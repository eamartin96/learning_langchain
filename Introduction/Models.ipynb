{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec560fd-a867-41b2-a077-6cd700afaaed",
   "metadata": {},
   "source": [
    "# LangChain Models\n",
    "[**LLMs**](https://en.wikipedia.org/wiki/Large_language_model) are powerfull AI tools that can interpret and generate text like humans. They're verstile enough to write content, translate language, summarize, and answer questions without needing specialized training for each task.\n",
    "\n",
    "[**LangChain documentation about models**](https://docs.langchain.com/oss/python/langchain/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c558b-46b6-45de-a4a1-e13b9bb7e0a0",
   "metadata": {},
   "source": [
    "## Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68593340-8f37-443b-a810-a2de4c9fed33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc82e8fa-abcb-46b9-b287-144de96bf2c1",
   "metadata": {},
   "source": [
    "## Basic Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c15a6d0-e354-4710-a4b3-7fae761c30da",
   "metadata": {},
   "source": [
    "### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c706f38-f7d5-4c26-a44b-6b073d07bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2199c3e2-2400-45f8-9976-0469ac882a14",
   "metadata": {},
   "source": [
    "`init_chat_model` Initializes a chat model from any supported provider using a unified interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f823fd17-e59b-4cf2-8a2f-8122a09c2e9e",
   "metadata": {},
   "source": [
    "### Model Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf5984-2f14-441a-b52e-7687fc74ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\"Why do parrots talk?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db32428c-c126-4e8d-85ce-cfbfb96036c1",
   "metadata": {},
   "source": [
    "### Model Parameters\n",
    "A chat model takes paramters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard one includes:\n",
    "- `model`: The name of the specirfic model you want to use with a provider. It's required for the initialization.\n",
    "- `api_key`: The key required for authenticating with the model's provider.\n",
    "- `temperature`: Controls the randomess of the model's output. A higher number makes reponses more creative.\n",
    "- `max_tokens`: Limits the total number of tokens in the response, effectively controlling how long the output can be.\n",
    "- `timeout`: The maximum time (in seconds) to wait for a response from the model before cancelling the requests.\n",
    "- `max_retries`: The maximum number of attempts the system will make to resend a request if it fails due to issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67bd213-ce51-440f-98bd-94a6f1480c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    model = \"gpt-4\",\n",
    "    temperature = 0.7,\n",
    "    max_tokens = 200\n",
    ")\n",
    "\n",
    "model.invoke(\"Why do parrots talk?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b899e-12bf-4a75-9e85-8421195d4ea3",
   "metadata": {},
   "source": [
    "## Key methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594bd65-1dcf-41c8-946d-18048978398a",
   "metadata": {},
   "source": [
    "### Invoke\n",
    "The most straightforward way to call a model is to use `invoke()` with a single message or a list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3405aae4-a54b-4cd2-9cde-79417f650726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Parrots have colorful feathers as a result of their evolution. The vibrant colors help parrots attract mates, camouflage themselves from predators, communicate with each other, and even reflect harmful UV rays. The color of the feathers comes from pigments found in their food, which are then integrated into the growing feathers. Different pigments produce different colors, for instance, carotenoids produce red, orange, or yellow colors, while porphyrins can give a range of colors including green, red, and brown.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 102, 'prompt_tokens': 15, 'total_tokens': 117, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-CkvkNSZWb4KKb5cKc9JhOEehdkLwV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--0e41b7e3-be17-49a1-9ab8-5fa90fb03995-0' usage_metadata={'input_tokens': 15, 'output_tokens': 102, 'total_tokens': 117, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(\"Why do parrots have colorful feathers?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a35f6-dff3-4176-957f-7e7944a47ce4",
   "metadata": {},
   "source": [
    "### Stream\n",
    "Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly longer responses.\n",
    "\n",
    "Calling `stream()` returns an iterator that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e42cbe-2b34-4684-965f-cc6c00a159cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n",
    "    print(chunk.text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a2873-ae31-4fec-8121-532baf62e058",
   "metadata": {},
   "source": [
    "### Batch\n",
    "Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16786006-11ce-4a64-9641-8f6ab50c5630",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = model.batch([\n",
    "    \"Why do parrots have colorful feathers?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"What is quantum computing?\"\n",
    "])\n",
    "\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecfbe66-4539-43f3-a232-78fd8387b375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
